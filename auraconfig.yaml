# --- SYSTEM SETTINGS ---
system:
  log_level: INFO
  default_timeout: 120.0

# --- HARDWARE INVENTORY (mobile1) ---
models:
  # ----- Ollama (local HTTP) -----
  local_3070_qwen:
    provider: ollama
    endpoint: http://localhost:11434/api/generate
    model_name: qwen2.5-coder:7b
    hosting_tier: on-prem
    cost_per_1m_input: 0.0
    cost_per_1m_output: 0.0
    parameters:
      temperature: 0.1
      num_ctx: 4096

  # ----- llama.cpp (embedded, no Ollama needed) -----
  # Download a GGUF first:  aurarouter download-model --repo Qwen/Qwen2.5-Coder-7B-Instruct-GGUF --file qwen2.5-coder-7b-instruct-q4_k_m.gguf
  # local_llama_qwen:
  #   provider: llamacpp
  #   model_path: "C:/models/qwen2.5-coder-7b-q4_k_m.gguf"
  #   parameters:
  #     n_ctx: 4096
  #     n_gpu_layers: -1      # -1 = all layers to GPU
  #     n_batch: 512
  #     temperature: 0.1
  #     max_tokens: 2048

  # ----- Ollama on another machine -----
  # local_3090_deepseek:
  #   provider: ollama
  #   endpoint: http://192.168.1.50:11434/api/generate
  #   model_name: deepseek-coder-v2:lite
  #   parameters:
  #     num_ctx: 16384

  # ----- Google Gemini (cloud) -----
  cloud_gemini_flash:
    provider: google
    model_name: gemini-2.0-flash
    hosting_tier: cloud
    cost_per_1m_input: 0.10
    cost_per_1m_output: 0.40
    # Paste key directly, or use env_key to read from shell
    api_key: YOUR_API_KEY
    # env_key: GOOGLE_API_KEY

  cloud_gemini_pro:
    provider: google
    model_name: gemini-2.0-pro-exp
    hosting_tier: cloud
    cost_per_1m_input: 1.25
    cost_per_1m_output: 10.00
    api_key: YOUR_API_KEY

  # ----- Anthropic Claude (cloud) -----
  # cloud_claude_sonnet:
  #   provider: claude
  #   model_name: claude-sonnet-4-5-20250929
  #   env_key: ANTHROPIC_API_KEY
  #   parameters:
  #     max_tokens: 4096
  #     temperature: 0.1

# --- ROLES & ROUTING ---
# The router iterates each list until one model succeeds.
roles:
  # Who decides intent?
  router:
    - local_3070_qwen
    - cloud_gemini_flash

  # Who creates the architectural plan?
  reasoning:
    # - local_3090_deepseek
    - cloud_gemini_pro
    - cloud_gemini_flash

  # Who writes the code?
  coding:
    - local_3070_qwen
    - cloud_gemini_flash

  # Who validates output quality? (Optional â€” enables closed-loop execution)
  reviewer:
    - cloud_gemini_pro
    - cloud_gemini_flash

# --- EXECUTION SETTINGS ---
execution:
  max_review_iterations: 3   # Max review-correct cycles per task (0 = disable review)
